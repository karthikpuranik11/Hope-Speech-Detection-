{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hopespeech_multilingual_bert",
      "provenance": [],
      "authorship_tag": "ABX9TyO7vvWxx5FXIc6/E2o3tkWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikpuranik11/Hope-Speech-Detection-/blob/main/Hopespeech_multilingual_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp9ne18LI0ZD",
        "outputId": "7ff3fa5e-f485-4169-867c-2061f8b672f5"
      },
      "source": [
        "!pip install transformers==3.3.1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.3.1 in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (1.18.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.8.1rc2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (20.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.1.94)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGoQZFFYJlu8"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "cQaMUgiLKe2h",
        "outputId": "6c6a463e-2429-4f4d-9339-75535ca91a62"
      },
      "source": [
        "train=pd.read_csv('/content/tamil_hope_first_train.tsv', header=None, names=['tweets','label'], sep=\"\\t\")\r\n",
        "train['labels']=LabelEncoder().fit_transform(train['label'])\r\n",
        "train=train.drop(columns='label')\r\n",
        "train"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Realme india product</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I also don't have tiktok hello and allnBut I'm...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Thalaivare..neengale inum one plus mobile vach...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Annee varanda thondai.. corona virus affect pa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5views but 18likes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16155</th>\n",
              "      <td>7pm correcta erukum mg bro</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16156</th>\n",
              "      <td>Intha karutha mudija varaikum Neengalum ellark...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16157</th>\n",
              "      <td>And neenga adhiyavasiyam nu soldra apps like t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16158</th>\n",
              "      <td>Daii sekram mater ku vada</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16159</th>\n",
              "      <td>Bro put Redmi 8 gaming</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16160 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  tweets  labels\n",
              "0                                   Realme india product       2\n",
              "1      I also don't have tiktok hello and allnBut I'm...       0\n",
              "2      Thalaivare..neengale inum one plus mobile vach...       1\n",
              "3      Annee varanda thondai.. corona virus affect pa...       0\n",
              "4                                     5views but 18likes       1\n",
              "...                                                  ...     ...\n",
              "16155                         7pm correcta erukum mg bro       0\n",
              "16156  Intha karutha mudija varaikum Neengalum ellark...       1\n",
              "16157  And neenga adhiyavasiyam nu soldra apps like t...       1\n",
              "16158                          Daii sekram mater ku vada       0\n",
              "16159                             Bro put Redmi 8 gaming       0\n",
              "\n",
              "[16160 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Whp4CHwJOr8H",
        "outputId": "e042565c-246d-4137-e3a4-6890af97776a"
      },
      "source": [
        "val=pd.read_csv('/content/tamil_hope_first_dev.tsv', header=None, names=['tweets','label'], sep=\"\\t\")\r\n",
        "val['labels']=LabelEncoder().fit_transform(val['label'])\r\n",
        "val=val.drop(columns='label')\r\n",
        "val"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mg bro eve 6 o clock video post pannuga</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.00 pm bro plss</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bro ivan kitta sonna . Ivan change' pannidava ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Varnajalam Mini Crafts adhum crrct thaan Akka 🤣</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>எல்லா குழந்தைகளும் நல்ல எண்ணம்</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013</th>\n",
              "      <td>Share karo app use panlama</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2014</th>\n",
              "      <td>@R SWAGxOP kelattu kapothi maari nee pesathada...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015</th>\n",
              "      <td>Yow yaean yaa kelappi vidringa ipd</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>Hey bro. Ur doing amazing...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>@Haram Bhai Allah yaaru saamy nee??</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2018 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  labels\n",
              "0               Mg bro eve 6 o clock video post pannuga       0\n",
              "1                                      8.00 pm bro plss       2\n",
              "2     Bro ivan kitta sonna . Ivan change' pannidava ...       0\n",
              "3      @Varnajalam Mini Crafts adhum crrct thaan Akka 🤣       1\n",
              "4                        எல்லா குழந்தைகளும் நல்ல எண்ணம்       0\n",
              "...                                                 ...     ...\n",
              "2013                         Share karo app use panlama       1\n",
              "2014  @R SWAGxOP kelattu kapothi maari nee pesathada...       1\n",
              "2015                 Yow yaean yaa kelappi vidringa ipd       1\n",
              "2016                       Hey bro. Ur doing amazing...       2\n",
              "2017                @Haram Bhai Allah yaaru saamy nee??       1\n",
              "\n",
              "[2018 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMd3ZHtlPpLv"
      },
      "source": [
        "import pandas as pd\r\n",
        "from torch.utils.data import Dataset,DataLoader\r\n",
        "\r\n",
        "class RFDataset(Dataset):\r\n",
        "  def __init__(self,tweets,labels,tokenizer,max_len):\r\n",
        "    self.tweets = tweets\r\n",
        "    self.labels = labels\r\n",
        "    self.tokenizer = tokenizer\r\n",
        "    self.max_len = max_len\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.tweets)\r\n",
        "  \r\n",
        "  def __getitem__(self,item):\r\n",
        "    tweets = str(self.tweets[item])\r\n",
        "    labels = self.labels[item]\r\n",
        "\r\n",
        "    encoding = self.tokenizer.encode_plus(\r\n",
        "        tweets,\r\n",
        "        add_special_tokens=True,\r\n",
        "        max_length = self.max_len,\r\n",
        "        return_token_type_ids = False,\r\n",
        "        padding = 'max_length',\r\n",
        "        return_attention_mask= True,\r\n",
        "        return_tensors='pt',\r\n",
        "        truncation=True\r\n",
        "    )\r\n",
        "\r\n",
        "    return {\r\n",
        "        'tweets' : tweets,\r\n",
        "        'input_ids' : encoding['input_ids'].flatten(),\r\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(),\r\n",
        "        'labels' : torch.tensor(labels,dtype=torch.long)\r\n",
        "\r\n",
        "    }"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDdVdA1bQlUF",
        "outputId": "3fc4a089-574a-4741-bf6f-98d067ccdd08"
      },
      "source": [
        " \r\n",
        "print('Training set size:',train.shape)\r\n",
        "#Uncomment the next line when we have the test data\r\n",
        "#print('Testing set size:',test.shape)\r\n",
        "print('validation set size:',val.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size: (16160, 2)\n",
            "validation set size: (2018, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFA6pybHQwOX",
        "outputId": "0fa7ee61-797c-4140-d461-9c7fe9c788ba"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.utils import class_weight\r\n",
        "class_weights = class_weight.compute_class_weight('balanced',\r\n",
        "                                                  np.unique(train.labels.values),\r\n",
        "                                                  train.labels.values)\r\n",
        "class_weights"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.85137769, 0.68428184, 2.74689784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrS3mf4RQyy9"
      },
      "source": [
        "def create_data_loader(df,tokenizer,max_len,batch_size):\r\n",
        "  ds = RFDataset(\r\n",
        "      tweets = df.tweets.to_numpy(),\r\n",
        "      labels = df.labels.to_numpy(),\r\n",
        "      tokenizer = tokenizer,\r\n",
        "      max_len = max_len\r\n",
        "  )\r\n",
        "\r\n",
        "  return DataLoader(ds,\r\n",
        "                    batch_size = batch_size,\r\n",
        "                    shuffle = True,\r\n",
        "                    num_workers=4)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5HC7hmTQ1zg"
      },
      "source": [
        "from transformers import XLNetTokenizer,XLNetModel,AdamW,get_linear_schedule_with_warmup,AutoModel,AutoTokenizer\r\n",
        "device = 'cpu'\r\n",
        "PRE_TRAINED_MODEL_NAME = 'bert-base-multilingual-cased'\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_c7oQlUQ4ED"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "MAX_LEN = 128\r\n",
        "train_data_loader = create_data_loader(train,tokenizer,MAX_LEN,BATCH_SIZE)\r\n",
        "val_data_loader = create_data_loader(val,tokenizer,MAX_LEN,BATCH_SIZE)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtkIQTq3Q6-b"
      },
      "source": [
        "BERT_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELD76HMVQ9HQ"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "class RFClassifier(nn.Module):\r\n",
        "  def __init__(self, n_classes):\r\n",
        "    super(RFClassifier, self).__init__()\r\n",
        "    self.auto = AutoModel.from_pretrained('bert-base-multilingual-cased')\r\n",
        "    self.drop = nn.Dropout(p=0.4)\r\n",
        "    #self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\r\n",
        "    self.out1 = nn.Linear(self.auto.config.hidden_size, 128)\r\n",
        "    self.drop1 = nn.Dropout(p=0.4)\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "    self.out = nn.Linear(128, n_classes)\r\n",
        "  \r\n",
        "  def forward(self, input_ids, attention_mask):\r\n",
        "    _,pooled_output = self.auto(\r\n",
        "      input_ids=input_ids,\r\n",
        "      attention_mask=attention_mask\r\n",
        "    )\r\n",
        "    #output = self.relu(pooled_output)\r\n",
        "    output = self.drop(pooled_output)\r\n",
        "    output = self.out1(output)\r\n",
        "    output = self.relu(output)\r\n",
        "    output = self.drop1(output)\r\n",
        "    return self.out(output)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzOmQqNyRAqg"
      },
      "source": [
        "model = RFClassifier(3)\r\n",
        "model = model.to(device)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxxmEAKvRC5H"
      },
      "source": [
        "\r\n",
        "EPOCHS = 25\r\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\r\n",
        "total_steps = len(train_data_loader) * EPOCHS\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "  optimizer,\r\n",
        "  num_warmup_steps=0,\r\n",
        "  num_training_steps=total_steps\r\n",
        ")\r\n",
        "\r\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x444fs7R0oz"
      },
      "source": [
        "\r\n",
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\r\n",
        "    model = model.train()\r\n",
        "    losses = []\r\n",
        "    correct_predictions = 0\r\n",
        "\r\n",
        "    for data in data_loader:\r\n",
        "        input_ids = data['input_ids'].to(device)\r\n",
        "        attention_mask = data['attention_mask'].to(device)\r\n",
        "        labels = data['labels'].to(device)\r\n",
        "\r\n",
        "        outputs = model(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask\r\n",
        "            )\r\n",
        "        _, preds = torch.max(outputs, dim=1)\r\n",
        "        loss = loss_fn(outputs,labels)\r\n",
        "\r\n",
        "        correct_predictions += torch.sum(preds == labels)\r\n",
        "        losses.append(loss.item())\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\r\n",
        "        optimizer.step()\r\n",
        "        scheduler.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGTlOJP3R2rX"
      },
      "source": [
        "\r\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\r\n",
        "  model = model.eval()\r\n",
        "  losses = []\r\n",
        "  correct_predictions = 0\r\n",
        "  with torch.no_grad():\r\n",
        "    for d in data_loader:\r\n",
        "      input_ids = d[\"input_ids\"].to(device)\r\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "      labels = d[\"labels\"].to(device)\r\n",
        "      outputs = model(\r\n",
        "        input_ids=input_ids,\r\n",
        "        attention_mask=attention_mask\r\n",
        "      )\r\n",
        "      _, preds = torch.max(outputs, dim=1)\r\n",
        "      loss = loss_fn(outputs, labels)\r\n",
        "      correct_predictions += torch.sum(preds == labels)\r\n",
        "      losses.append(loss.item())\r\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvBZLbZAR4wN"
      },
      "source": [
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "hN5SVoUVR6kL",
        "outputId": "8c1b59f0-d230-4e9d-c408-4b31e223fdda"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "import torch\r\n",
        " \r\n",
        "history = defaultdict(list)\r\n",
        "best_accuracy = 0\r\n",
        "for epoch in range(EPOCHS):\r\n",
        " \r\n",
        " \r\n",
        "  start_time = time.time()\r\n",
        "  train_acc,train_loss = train_epoch(\r\n",
        "      model,\r\n",
        "      train_data_loader,\r\n",
        "      loss_fn,\r\n",
        "      optimizer,\r\n",
        "      device,\r\n",
        "      scheduler,\r\n",
        "      len(train)\r\n",
        "  )\r\n",
        "   \r\n",
        " \r\n",
        "#  val_acc,val_loss = eval_model(\r\n",
        "#      model,\r\n",
        "#      val_data_loader,\r\n",
        "#      loss_fn,\r\n",
        "#      device,\r\n",
        "#      len(val)\r\n",
        "#  )\r\n",
        "\r\n",
        "  end_time = time.time()\r\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "  print(f'Train Loss {train_loss} accuracy {train_acc}')\r\n",
        " # print(f'Val Loss {val_loss} accuracy {val_acc}')\r\n",
        "  print()\r\n",
        "\r\n",
        "  history['train_acc'].append(train_acc)\r\n",
        "  history['train_loss'].append(train_loss)\r\n",
        "#  history['val_acc'].append(val_acc)\r\n",
        "#  history['val_loss'].append(val_loss)\r\n",
        "\r\n",
        "  if train_acc > best_accuracy:\r\n",
        "    torch.save(model.state_dict(),'multilingual-bert-base-uncased(25_Epochs).bin')\r\n",
        "    best_accuracy = train_acc"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7f7761e94d00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-1c8ad2ec219f>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     15\u001b[0m             )\n\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXpyt4kPR8Qo"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.plot(history['train_acc'], label='train accuracy')\r\n",
        "#plt.plot(history['val_acc'], label='validation accuracy')\r\n",
        "plt.title('Training history')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "#plt.legend()\r\n",
        "#plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4jnJ_DeVChr"
      },
      "source": [
        "val_acc, _ = eval_model(\r\n",
        "  model,\r\n",
        "  val_data_loader,\r\n",
        "  loss_fn,\r\n",
        "  device,\r\n",
        "  len(val) #Change it to test when you have the test results\r\n",
        ")\r\n",
        "val_acc.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrOtx2a1VFPV"
      },
      "source": [
        "def get_predictions(model, data_loader):\r\n",
        "  model = model.eval()\r\n",
        "  sentence = []\r\n",
        "  predictions = []\r\n",
        "  prediction_probs = []\r\n",
        "  real_values = []\r\n",
        "  with torch.no_grad():\r\n",
        "    for d in data_loader:\r\n",
        "      tweets = d[\"tweets\"]\r\n",
        "      input_ids = d[\"input_ids\"].to(device)\r\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\r\n",
        "      labels = d[\"labels\"].to(device)\r\n",
        "      outputs = model(\r\n",
        "        input_ids=input_ids,\r\n",
        "        attention_mask=attention_mask\r\n",
        "      )\r\n",
        "      _, preds = torch.max(outputs, dim=1)\r\n",
        "      sentence.extend(tweets)\r\n",
        "      predictions.extend(preds)\r\n",
        "      prediction_probs.extend(outputs)\r\n",
        "      real_values.extend(labels)\r\n",
        "  predictions = torch.stack(predictions).cpu()\r\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\r\n",
        "  real_values = torch.stack(real_values).cpu()\r\n",
        "  return sentence, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A6ySmh7VNeo"
      },
      "source": [
        "\r\n",
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\r\n",
        "  model,\r\n",
        "  val_data_loader\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdnRltryVRuD"
      },
      "source": [
        "class_name = ['Hope_speech','Non_hope_speech','not-Tamil']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AbMa4soViJ-"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\r\n",
        "print(classification_report(y_test, y_pred, target_names=class_name,zero_division=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twdTb-qhVlbe"
      },
      "source": [
        "import seaborn as sns\r\n",
        "def show_confusion_matrix(confusion_matrix):\r\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\r\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\r\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\r\n",
        "  plt.ylabel('True sentiment')\r\n",
        "  plt.xlabel('Predicted sentiment');\r\n",
        "cm = confusion_matrix(y_test, y_pred)\r\n",
        "df_cm = pd.DataFrame(cm, index=class_name, columns=class_name)\r\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
